---
title: "Assigment - Naive Bayes DIY"
author:
  - name author here - JuniorB2
  - name reviewer here - TICE97
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_notebook:
    toc: true
    toc_depth: 2
---



```{r}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
```


Choose a suitable dataset from [this](https://github.com/HAN-M3DM-Data-Mining/assignments/tree/master/datasets) folder and train your own Naive Bayes model. Follow all the steps from the CRISP-DM model.


## Business Understanding
I chose the fakenews assignment. The dataframe is more workable than the hate speech file, in this dataframe every article is labeled with 1: unreliable and 0: reliable. 
With the hate speech it has a hate speech index but the posts with no hatespeech are labeled with "NA" and i thought that was more difficult.

## Data Understanding


```{r}
#we download the csv file
rawDF <- read.csv("https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/data-mining-s2y2122-JuniorB2/master/datasets/NB-fakenews.csv", header = TRUE)
head(rawDF)
```


```{r}
rawDF$label <- rawDF$label %>% factor %>% relevel("1")
head(rawDF, 10)
class(rawDF$label)
table(rawDF$label)
#we make the label column a factor here
```


```{r}
s <- sample(c(1:dim(rawDF)[1], 5000))
unreliable <- rawDF[s,] %>% filter(label == "1")
reliable <- rawDF[s,] %>% filter(label == "0")

```

```{r}
wordcloud(unreliable$text, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud(reliable$text, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))

#unreliable- words: trump - clinton - will - people - hillary - said - the
#reliable- words: The - Said - president - years - trump - people - new - time
```



## Data Preparation
```{r}
rawCorpus <- Corpus(VectorSource(rawDF$text))
inspect(rawCorpus[1:3])
#the raw corpus
```

```{r}
cleancorpus <- rawCorpus %>% tm_map(tolower) %>% tm_map(removeNumbers) 

cleancorpus <- cleancorpus %>% tm_map(tolower) %>% tm_map(removeWords, stopwords()) %>% tm_map(removePunctuation)

cleancorpus <- cleancorpus %>% tm_map(stripWhitespace)
cleanDTM <- cleancorpus %>% DocumentTermMatrix()
inspect(cleanDTM)

freqWords <- cleanDTM %>% findFreqTerms(30)
cleanDTM2 <- DocumentTermMatrix(cleancorpus, list(dictionary = freqWords))
inspect(cleanDTM2)
#here we clean the raw corpus and wipe all the punctuation and more
#it also creates Document term matrix (DTM) that counts all the words, that is why it takes so long
```



```{r}
set.seed(1234)
trainIndex <- createDataPartition(rawDF$label, p=.75, list = FALSE, times = 1)
head(trainIndex)

trainDF <- rawDF[trainIndex, ]
testDF <- rawDF[-trainIndex, ]

trainCorpus <- cleancorpus[trainIndex]
testCorpus <- cleancorpus[-trainIndex]
trainDTM <- cleanDTM2[trainIndex, ]
testDTM <- cleanDTM2[-trainIndex, ]
```



```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("no", "yes"))
}

nColsDTM <- dim(trainDTM)[2]
trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)

head(trainDTM[,1:10])
#This creates a matrix with every word and the rows are the messages.
# if a word is in the message it will say "yes"
```

# modeling
```{r}
nbayesModel <-  naiveBayes(trainDTM, trainDF$label, laplace = 1)
predVec <- predict(nbayesModel, testDTM)
confusionMatrix(predVec, testDF$label, positive = "1", dnn = c("Prediction", "True"))
#this is the final step of modeling and looking at the outcome
```



## Evaluation and Deployment
reviewer adds suggestions for improving the model

At first the file looks normal, we download the csv file and we change the column:"label" to a factor. From then on we follow every step as demonstrated on the online workbook, we make a word cloud and after that we start preparing the data.

We create a raw corpus and after that we start cleaning the data, we take out the numbers, capital letters,  excessive white spaces and punctuation. After that we create 2 DTM's, one of the clean corpus and another where we use the tm package to filter out the words with a low frequency. 

Then we start splitting the datasets and here i found the first mistake, here you see the code that is used to split the data:
```{r}
set.seed(1234)
trainIndex <- createDataPartition(rawDF$label, p=.90, list = FALSE, times = 1)
head(trainIndex)
```
The p is set to .90, which means that the dataset is split 90/10%
This creates a unreliable test set, it should be set to p=.75
Here is the correct code:
```{r}
set.seed(1234)
trainIndex <- createDataPartition(rawDF$label, p=.75, list = FALSE, times = 1)
head(trainIndex)
```

After changing this the file did work (it didn't work before because of the split size), i think the only recommendations i would make is changing the findFreqterms to a higher number. This will filter more words and i think it would be more reliable. What i also would do is change the labels to names, now in the confusionMatrix you only see a 1 and a 0, if there was a way to change that to "Unreliable/Fake" and "Reliable" then the matrix would be easier to understand.

#Questions
1. What do you think is the role of the laplace parameter in the naiveBayes() function?
2. How would you assess the overall performance of the model?
3. What would you consider as more costly: high false negatives or high false positives levels? Why?

#Awnsers 
1. This is related to the video we watched, in the video they talk about the probability a word is occurring in an email, but if that word is never counted then the probability is always zero. The laplace function takes care of that problem.
2. I think it is alright, the accuracy is only 72% which is fine but if you look at the confusion matrix there are a lot of false negatives and false positives and i think that is a bit of a problem.
3. In this case the FN = an article that is classified as reliable when it actually isn't 
The FP = an article that is classified as unreliable when it actually is reliable.
both cases are very costly, we're talking about information so you can't afford any mistake. Also if you count both cases and add them you get 1426 articles that are not calculated correctly, that is almost as high as the number of true negatives. I would say that both cases are very costly.





